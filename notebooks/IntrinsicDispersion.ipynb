{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==  Basic import == #\n",
    "# plot within the notebook\n",
    "%matplotlib inline\n",
    "# No annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic dispersion\n",
    "**Likelihood minimization of Gaussian Distribution**\n",
    "\n",
    "The goal of this exercise is the fill the `WeightedMean` class such that the code works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma_int = 0.10 # Intrinsic dispersion to recover\n",
    "mu = -0.5 # Mean value to recover\n",
    "error = 0.12 # Tyipical noise of the data\n",
    "error_noise = 0.03 # error of the noise (so that are not always the same) will be 0.12 +/- 0,03\n",
    "npoints = 1000 # numper of points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fake data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors = np.random.normal(loc=error, scale=error_noise, size=npoints)\n",
    "data   = np.random.normal(loc=mu, scale=sigma_int, size=npoints) + np.random.normal(loc=0,scale=errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = mpl.figure(figsize=[13,5])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(np.arange(npoints), data, yerr=errors, marker=\"o\", ls=\"None\",\n",
    "            mfc=mpl.cm.Blues(0.6,0.6), mec=mpl.cm.binary(0.8,1), ecolor=\"0.7\",\n",
    "           ms=13, mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula to adjust\n",
    "\n",
    "** Some Theory to understand what is going on**\n",
    "\n",
    "## The Probability to measure _x_ with and error _dx_\n",
    "\n",
    "#### The Theory\n",
    "The probability _p_ to observe a point _i_ with a value _x_ given it's gaussian error _dx_ and assuming the system has an intrinsic dispersion _sigma_ is:\n",
    "\n",
    "$$\n",
    "p = G(x_i\\ |\\ \\mu,\\ \\sqrt{dx_i ^2+ sigma^2})\n",
    "$$\n",
    "where $G$ is the gaussian probability distribution function (pdf).\n",
    "\n",
    "#### The Code\n",
    "In Python you can measure $p$ using the scipy.stats norm class:\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "p = norm.pdf(x, loc= mu, scale= np.sqrt(dx**2 + sigma**2))\n",
    "```\n",
    "\n",
    "## The Likelihood of your sample\n",
    "\n",
    "The likelihood to observe your sample given your model (here $\\mu$ and $\\sigma$) is simply the product of the \n",
    "probability to observe every given points of your sample. The best model with then be the one maximizing the Likelihood $\\mathcal{L}$:\n",
    "$$\n",
    "\\mathcal{L} = \\prod p_i\n",
    "$$\n",
    "\n",
    "In practive we work with the log of the likelihood such that the formula is based on the sum of the log of the individual probabilities:\n",
    "\n",
    "$$\n",
    "\\log\\mathcal{L} = \\sum \\log(p_i)\n",
    "$$\n",
    "\n",
    "\n",
    "You should be able to code the `minus_loklikelihood` ($=-\\log\\mathcal{L}$) [remark that scipy.stats.norm may take vectors to speed things up]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "class WeightedMean( object ):\n",
    "    \"\"\" Class allowing to fit a weighted mean including intrinsic dispersion\"\"\"\n",
    "        \n",
    "    PARAMETERS = [mu, sigma_int]\n",
    "    \n",
    "    def __init__(self, data, errors):\n",
    "        \"\"\" Initialize the class \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: [array]\n",
    "            measured value\n",
    "        errors: [array]\n",
    "            measured errors\n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        Void\n",
    "        \"\"\"\n",
    "\n",
    "        # => Input Test: Data and Errors must have the same size\n",
    "        # => Create self.data, self.errors (they should be numpy array)\n",
    "        # => Create  self.npoints, which is just the number of data point\n",
    "        \n",
    "    def set_guesses(self,guesses):\n",
    "        \"\"\" Set the 2 guesses for the fit: mu and sigma_intrinsic\n",
    "        Return\n",
    "        ------\n",
    "        Void\n",
    "        \"\"\"\n",
    "        # =Input Test: guesses must have the same size as the number of parameters\n",
    "        # => Create self.guesses recording the numpy array of guesses\n",
    "\n",
    "        \n",
    "    def fit(self, guesses=[0,1], verbose=True):\n",
    "        \"\"\" fit the parameters to the data\n",
    "        This uses scipy.optimize.minimize\n",
    "        Parameters:\n",
    "        -----------\n",
    "        guesses: [array]\n",
    "            Initial values for the fit.\n",
    "        verbose: [bool]\n",
    "            print the results if True\n",
    "        Return\n",
    "        ------\n",
    "        Void (creates self.fitout)\n",
    "        \"\"\"\n",
    "        # => Set the guesses using the appropriate function\n",
    "        # => fill the \"***\" in the following method\n",
    "        \n",
    "        self.fitout = minimize(\"***\", \"***\")\n",
    "        if verbose:\n",
    "            print self.fitout\n",
    "            \n",
    "        \n",
    "    def minus_loglikelihood(self, parameters):\n",
    "        \"\"\" The sum of the minus loglikelihood used for the fit\n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters: [array]\n",
    "            list of the values for the free parameters of the model\n",
    "        Return\n",
    "        ------\n",
    "        float (- sum loglikelihood)\n",
    "        \"\"\"\n",
    "        mu, sigma_int = parameters\n",
    "        #=> return the - sum of the log of the individual likelihood (using stats.norm.pdf)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run that, it should work\n",
    "\n",
    "The \"x\" entry is the fitted parameters and hess_inv is an approximation of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wmean = WeightedMean(data,errors)\n",
    "wmean.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
